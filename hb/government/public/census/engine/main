#!/usr/bin/env ruby

require 'yaml'
require 'fileutils'
require 'sqliteshapedb'
require 'engine/calc_demographic_properties'

# inputdir is the first argument your main script will get. It will ALWAYS get
# this. inputdir will ALWAYS be a directory that contains (ripd/, rawd/, fixd/,
# and log/).
inputdir  = ARGV[0]

# outputdir is the second argument your main script will get. It will ALWAYS get
# this. outputdir will always be a directory that contains (env/). In env/ is
# where you will find the yaml file containing all the configuration settings
# from ../config/config.yaml.
outputdir = ARGV[1]

# Example. Reads the yaml file into a ruby hash (same as a javascript
# associative array, a java hashmap, a python dictionary, etc) called 'opts'

opts = JSON.parse(File.read File.join(outputdir, "env", "working_environment.json"))
#opts = { 'ftp' => { 'data_dir' => 'census_2010/03-Demographic_Profile' }, 'data_assets' => [{ 'location'=> 'data/census2010_data.tsv'}] }

# Open some log files
$xref_log = File.open("#{inputdir}/log/xref.log", "w")


def import_demographics_file(path)
  demoHash = Hash.new
  File.foreach(path) { |record|
    fields = record.split(",")
    logrecno = fields[4]
    demoHash[logrecno] = calc_demographic_properties(fields)
  }
  demoHash
end

def calculate_geo_key(line)
  state_fips = line[27..28].strip
  county_fips = line[29..31].strip
  cousub_fips = line[36..40].strip
  place_fips = line[45..49].strip
  aianhh = line[76..79].strip
  aianhhcc = line[85..86].strip
  anrc = line[105..109].strip
  cbsa = line[112..116].strip
  metdiv = line[119..123].strip
  csa = line[124..126].strip
  necta = line[127..131].strip
  cd = line[153..154].strip
  sldu = line[155..157].strip
  sldl = line[158..160].strip
  lsadc = line[359..360]
  geo_key = lsadc + case lsadc
    when "M0"
      csa
    when "M1", "M2"
      cbsa
    when "M3"
      cbsa + metdiv
    when "M5", "M6"
      necta
    when "03", "04", "05", "06", "12", "13", "15"
      state_fips + county_fips
    when "LU"
      state_fips + sldu
    when "LL"
      state_fips + sldl
    when /^C\d/
      state_fips + cd
    when "21", "25", "37", "47", "53", "55", "57", "62"
      state_fips + place_fips
    when "39", "43", "44", "45", "49"
      if cousub_fips.empty?
        state_fips + place_fips
      else
        state_fips + county_fips + cousub_fips
      end
    when "77"
      state_fips + anrc
    when "78", "79", /^[89]\d$/
      # "81", "82", "85", "86", "87", "89", "91", "92", "96", "97", "98"
      aianhh + (aianhhcc=="D3" || aianhhcc=="F1" ? "T" : "R")
    else
      state_fips + (county_fips+csa+cd+sldu+sldl)
  end
  geo_key
end

def import_geo_file(path)
  $xref_log.puts "PATH: #{path}"
  geoHash = Hash.new
  File.foreach(path) { |line|
    logrecno = line[18..24]
    name = line[226..315].strip
    geoHash[logrecno] = { :name => name, :geo_key => calculate_geo_key(line) }
  }
  geoHash
end

def cross_reference(demoHash, geoHash, &b)
  lsadc_not_found = Array.new
  demoHash.each { |logrecno,properties|
    name = geoHash[logrecno][:name]
    geo_key = geoHash[logrecno][:geo_key]
    xref_name,geojson = SqliteShapeDB.find_shape(geo_key)
    if geojson.nil?
      lsadc_not_found.push(geo_key[0..1])
      $xref_log.puts "  geo_key '#{geo_key}' not found for #{name}"
    else
      feature_json = "{\"type\":\"Feature\",\"geometry\":#{geojson},\"properties\":#{properties.to_json}}"
      yield(geo_key, xref_name, feature_json)
    end
  }
  $xref_log.puts "  LSADC not found: #{lsadc_not_found.sort.uniq.join(" ")}"
end

def generate_geojson(tsvfile, demoHash, geoHash)
  cross_reference(demoHash, geoHash) { |geo_key,name,feature_json|
    tsvfile.print "demo.census\t#{geo_key}\t#{name}\t"
    tsvfile.print feature_json
    tsvfile.puts
  }
end

def process_dp_dir(tsvfile, dp_dir)
  dpfile = Dir[File.join(dp_dir, "??[0-9]*.dp")].first
  geofile = Dir[File.join(dp_dir, "??geo*.dp")].first
  demoHash = import_demographics_file(dpfile)
  geoHash = import_geo_file(geofile)
  generate_geojson(tsvfile, demoHash, geoHash)
end

# Do the work
SqliteShapeDB.open_db(File.join(inputdir, "rawd", "shapes.sqlite"))
data_dir = File.join(inputdir, "ripd", opts['ftp']['data_dir'])
tsv_filename = File.join(outputdir, opts["data_assets"][0]["location"])
FileUtils.mkdir_p(File.dirname(tsv_filename))
File.open(tsv_filename, "w") { |tsvfile|
  dp_dirs = Dir[File.join(data_dir, "**")].sort
  dp_dirs.each { |dp_dir|
    puts "Processing #{dp_dir} ..."
    process_dp_dir(tsvfile, dp_dir)
    $xref_log.flush
  }
}

$xref_log.close
puts "\n\nMain script is done"
