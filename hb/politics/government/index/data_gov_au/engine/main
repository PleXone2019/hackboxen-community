#!/usr/bin/env ruby

require 'nokogiri'
require 'rubygems'
require 'iconv'
require 'rake'
require 'open-uri'

$cathash={"Business"=>"economics",
  "Communication"=>"socialsciences-communications",
  "Community"=>nil,
  "Culture"=>"artsandculture",
  "Education"=>"socialsciences-education",
  "Emergencies"=>"medicine-publichealth",
  "Employment"=>"economics-laborandemployment",
  "Environment"=>"geography",
  "Finance"=>"economics-finance",
  "General"=>nil,
  "Geography"=>"geography",
  "Government"=>"politicsandlaw",
  "Health"=>"medicine-publichealth",
  "Industry"=>"economics",
  "Information-communications-technologies"=>"computers-internet",
  "Law"=>"politicsandlaw-law",
  "Measurement"=>nil,
  "Planning"=>nil,
  "Politics"=>"politicsandlaw",
  "Property"=>"geography",
  "Recreation"=>"sports",
  "Safety"=>"medicine-publichealth",
  "Sciences"=>"science",
  "Society"=>"socialsciences-sociology",
  "Technology"=>"science",
  "Tourism"=>"economics-trade",
  "Transport"=>"engineering-transportation" }

$inputdir  = ARGV[0]
$outputdir = ARGV[1]

task :process_pages do
  FileUtils.mkdir_p(File.join($outputdir,"data"))
  #If we already have some of the pages processed, figure out which pages we've processed.
  $final_array=[]
  outfile=File.join($outputdir,"catalog.yaml")
  prev_data=YAML::load(File.open(outfile).read) if File.exist?(outfile)
  if prev_data
    puts "There were #{prev_data.size} datasets already in the catalog, and #{Dir[File.join($inputdir,"ripd/*")].size} total downloaded datasets."
    prev_data.map!{|x| x=x['catalog name']}
    Dir[File.join($inputdir,"ripd/*")].each {|file|
      $final_array.push(process_page file) unless prev_data.index(File.basename(file)) }
  else
    puts "There are #{Dir[File.join($inputdir,"ripd/*")].size} total downloaded datasets."
    Dir[File.join($inputdir,"ripd/*")].each {|file|
      $final_array.push(process_page file)}
  end
  puts "We added #{$final_array.size} datasets to the catalog."
  buffer=$final_array.to_yaml[5..-1]
  ic = Iconv.new("UTF-8//TRANSLIT", "LATIN1")
  File.open(outfile,'a').write(ic.iconv(buffer))

  puts "There are now #{(Dir[File.join($outputdir,"data/*")]/2).to_s} downloaded datasets."
  #Also consider loading in the final array, adding everything, uniqing, and sorting. Seems like it would be simpler to code but longer to run, and I already wrote this code.
end

def process_page page
  #Gather data
  doc = Nokogiri::HTML(File.open(page))
  name=doc.xpath('//meta[@name="DCTERMS.Title"]')[0].attribute('content').to_s
  desc=doc.xpath('//meta[@name="DCTERMS.Description"]')[0].attribute('content').to_s
  tags=doc.xpath('//div[@class="entry"]//dd[@property="dc:keywords"]').children.map{|x| x.children.to_s}.delete_if{|n| n==""}
  cats=doc.xpath('//div[@class="entry"]//dd[@property="dcat:theme"]').children.map{|x| $cathash[x.children.to_s]}.delete_if{|n| !n}
  rat=doc.xpath('//div[starts-with(@id,"gdsr_mur_text")]').to_s[/\d\.\d/]
  #There might be multiple dataset links. We want to pull down only one, with
  #extension preference csv>txt>xls>other.
  links,sizes=[],[]
  doc.xpath('//a[@property="dcat:accessURL"]').each{|x| links.push(x.attribute('href').to_s[5..-1])}
  doc.xpath('//span[@property="dc:format"]').each{|x| sizes.push(x.next.children.to_s)}
  if links.size>1
    ext=links.map{|x| File.extname(x)}
    if ext.index(".csv")
      sizes=sizes[ext.index(".csv")]
      links=links[ext.index(".csv")]
    elsif ext.index(".txt")
      sizes=sizes[ext.index(".txt")]
      links=links[ext.index(".txt")]
    elsif ext.index(".xls")
      sizes=sizes[ext.index(".xls")]
      links=links[ext.index(".xls")]
    else
      sizes=sizes[0]
      links=links[0]
    end
  else
    sizes=sizes[0]
    links=links[0]
  end
  sour=doc.xpath('//dd[@property="dc:creator"]').children[0].to_s.lstrip
  
  if links and not links.empty? and not File.extname(links)[0..3]=".htm"
    #Generate ICSS
    bname = File.basename(page)
    icss = File.open(File.join($inputdir,"rawd","icss.yaml")).read
    icss.gsub!("data_gov_au",bname)
    data_name=bname+File.extname(links)
    icss.gsub!("name_of_dataset.tsv",data_name)
    icss.gsub!("Name Of Dataset Entry",name)
    icss.gsub!("- tags","- " << tags.join("\n    - "))
    icss_desc="A #{sizes} dataset from data.gov.au. #{desc}"
    #      * Size: if applicable
    #      * Type: if applicable
    #      * Desc: if applicable
    #      * Sexy: Yes, definitely"
    icss.gsub!("DESCDESCDESC",icss_desc)
    path = File.join($outputdir,"data",bname+".icss.json")
    ic = Iconv.new("UTF-8//TRANSLIT", "LATIN1")
    File.new(path,'w').write(ic.iconv(icss)) unless File.exist?(path)
    
    #Pull down dataset
    begin
      path = File.join($outputdir,"data",data_name)
      File.new(path,'w').write(open(links).read) unless File.exist?(path)
    rescue
      puts "There was an error pulling down #{data_name} from #{links}."
    end
  end
  #Add to catalog
  return Hash["title",name,"description",desc,"tags",tags,"category list",cats,"overall_rating",rat,"link",links,"dataset size",sizes,"source",sour,"catalog name",File.basename(page)]
end

task :catalog_icss do
  icss = File.open(File.join($inputdir,"rawd","icss.yaml")).read
  icss.gsub!("data/name_of_dataset.tsv","catalog.yaml")
  icss.gsub!("au_entry","au_catalog")
  icss.gsub!("Name Of Dataset Entry from data.gov.au","data.gov.au Dataset Catalog")
  icss.gsub!("- tags","- government\n    - australia")
  icss_desc="The Australian government posts datasets online for public use, and there are currently 270 datasets available. All of the datasets which are a single file are also hosted here on infochimps, but this catalog contains basic metadata about all of the datasets available, including links to websites which can't be easily downloaded."
  icss.gsub!("DESCDESCDESC",icss_desc)
  File.new(File.join($outputdir,"catalog.icss.json"),'w').write(icss) #Always update the catalog icss.
end

task :run => [:process_pages, :catalog_icss]

Rake::Task[:run].invoke
