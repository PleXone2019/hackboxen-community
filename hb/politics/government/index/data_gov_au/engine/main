#!/usr/bin/env ruby

require 'rubygems'
require 'nokogiri'
require 'rubygems'
require 'iconv'
require 'rake'
require 'open-uri'
require 'json'

$cathash={"Business"=>"economics",
  "Communication"=>"socialsciences-communications",
  "Community"=>nil,
  "Culture"=>"artsandculture",
  "Education"=>"socialsciences-education",
  "Emergencies"=>"medicine-publichealth",
  "Employment"=>"economics-laborandemployment",
  "Environment"=>"geography",
  "Finance"=>"economics-finance",
  "General"=>nil,
  "Geography"=>"geography",
  "Government"=>"politicsandlaw",
  "Health"=>"medicine-publichealth",
  "Industry"=>"economics",
  "Information-communications-technologies"=>"computers-internet",
  "Law"=>"politicsandlaw-law",
  "Measurement"=>nil,
  "Planning"=>nil,
  "Politics"=>"politicsandlaw",
  "Property"=>"geography",
  "Recreation"=>"sports",
  "Safety"=>"medicine-publichealth",
  "Sciences"=>"science",
  "Society"=>"socialsciences-sociology",
  "Technology"=>"science",
  "Tourism"=>"economics-trade",
  "Transport"=>"engineering-transportation" }

$inputdir  = ARGV[0]
$outputdir = ARGV[1]

task :process_pages do
  FileUtils.mkdir_p(File.join($outputdir,"data"))
  FileUtils.cd(File.join($outputdir,"data"))
  #If we already have some of the pages processed, figure out which pages we've processed.
  $final_array=[]
  outfile=File.join($outputdir,"catalog.yaml")
  prev_data=YAML::load(File.open(outfile).read) if File.exist?(outfile)
  if prev_data
    puts "There were #{prev_data.size} datasets already in the catalog, and #{Dir[File.join($inputdir,"ripd/*")].size} total downloaded datasets."
    prev_data.map!{|x| x=x['catalog name']}
    Dir[File.join($inputdir,"ripd/*")].each {|file|
      $final_array.push(process_page file) unless prev_data.index(File.basename(file))
      $final_array.pop unless $final_array[-1]}
  else
    puts "There are #{Dir[File.join($inputdir,"ripd/*")].size} datasets to download."
    Dir[File.join($inputdir,"ripd/*")].each {|file|
      $final_array.push(process_page file)
      $final_array.pop unless $final_array[-1]}
  end
  puts "We added #{$final_array.size} datasets to the catalog."
  buffer=$final_array.to_yaml[5..-1]
  #ic = Iconv.new("UTF-8//TRANSLIT", "LATIN1")
  #File.open(outfile,'a').write(ic.iconv(buffer))
  File.open(outfile,'a').write(buffer)
  
  puts "There are now #{(Dir[File.join($outputdir,"data/*")].size/2).to_s} downloaded datasets."
  #Also consider loading in the final array, adding everything, uniqing, and sorting. Seems like it would be simpler to code but longer to run, and I already wrote this code.
end

def process_page page
  #Gather data
  doc = Nokogiri::HTML(File.open(page))
  name=doc.xpath('//meta[@name="DCTERMS.Title"]')[0].attribute('content').to_s
  desc=doc.xpath('//meta[@name="DCTERMS.Description"]')[0].attribute('content').to_s
  tags=doc.xpath('//div[@class="entry"]//dd[@property="dc:keywords"]').children.map{|x| x.children.to_s}.delete_if{|n| n==""}
  cats=doc.xpath('//div[@class="entry"]//dd[@property="dcat:theme"]').children.map{|x| $cathash[x.children.to_s]}.delete_if{|n| !n}
  rat=doc.xpath('//div[starts-with(@id,"gdsr_mur_text")]').to_s[/\d\.\d/]
  #There might be multiple dataset links. We want to pull down only one, with
  #extension preference csv>txt>xls>other.
  links,sizes=[],[]
  doc.xpath('//a[@property="dcat:accessURL"]').each{|x| links.push(x.attribute('href').to_s[5..-1])}
  doc.xpath('//span[@property="dc:format"]').each{|x| sizes.push(x.next.children.to_s)}
  if links.size>1
    ext=links.map{|x| File.extname(x)}
    if ext.index(".csv")
      sizes=sizes[ext.index(".csv")]
      links=links[ext.index(".csv")]
    elsif ext.index(".txt")
      sizes=sizes[ext.index(".txt")]
      links=links[ext.index(".txt")]
    elsif ext.index(".xls")
      sizes=sizes[ext.index(".xls")]
      links=links[ext.index(".xls")]
    else
      sizes=sizes[0]
      links=links[0]
    end
  else
    sizes=sizes[0]
    links=links[0]
  end
  sour=doc.xpath('//dd[@property="dc:creator"]').children[0].to_s.lstrip
  if links and not links.empty? and not File.extname(links)[0..3]==".htm"
    #Generate ICSS
    #The file can't start with a number, or contain characters other than underscores. gsub to the rescue!
    bname = File.basename(page).gsub('-',"_")
    #If it starts with a number, move it to the back.
    lead_num = bname[/\A\d.+\d/]
    bname = bname[lead_num.size+1..-1] << "-" << bname[0..lead_num.size-1] if lead_num
    #Add in the extension.
    data_name=bname+File.extname(links)
    #Write the ICSS.
    path = File.join($outputdir,"data",bname+".icss.json")
    unless File.exist?(path)
      icss_string = File.open(File.join($inputdir,"rawd","icss.json")).read
      icss = JSON.parse(icss_string)
      icss["data_assets"][0]["name"]=bname+"_data_asset"
      icss["data_assets"][0]["name"]=data_name
      icss["targets"]["catalog"][0]["title"].gsub!("Name Of Dataset Entry",name)
      icss["targets"]["catalog"][0]["tags"]=tags
      icss["targets"]["catalog"][0]["description"]="A #{sizes} dataset from data.gov.au. #{desc}"
      #ic = Iconv.new("UTF-8//TRANSLIT", "LATIN1")
      #File.new(path,'w').write(ic.iconv(icss.inspect))
      File.new(path,'w').write(icss.inspect)
    end
    #Pull down dataset
    begin
      path = File.join($outputdir,"data",data_name)
      links=URI.encode links if links.index(" ")
      sh "wget -O #{path} #{links}" unless File.size?(path)
#      File.new(path,'w').write(open(links).read) unless File.size?(path)
      #For debug, only pull down the first 1024 bytes.
      #File.new(path,'w').write(open(links).read(10)) unless File.exist?(path)
    rescue
      puts "There was an error pulling down #{data_name} from #{links}."
      path = File.join($outputdir,"data",data_name)
      #This might be unnecessary, but I want to make sure we don't eat good data if we get here accidentally.
      unless File.size?(path)
        File.delete(path) if File.exist?(path)
        path = File.join($outputdir,"data",bname+".icss.json")
        File.delete(path) if File.exist?(path)
        return nil
      end
    end
  end
  #Add to catalog
  return Hash["title",name,"description",desc,"tags",tags,"category list",cats,"overall_rating",rat,"link",links,"dataset size",sizes,"source",sour,"catalog name",File.basename(page)]
end

task :catalog_icss do
  icss_string = File.open(File.join($inputdir,"rawd","icss.json")).read
  icss = JSON.parse(icss_string)
  icss["data_assets"][0]["name"]="catalog.yaml"
  icss["targets"]["catalog"][0]["title"]="data.gov.au Dataset Catalog"
  icss["targets"]["catalog"][0]["tags"]=["government","Australia","public data"]
  icss["targets"]["catalog"][0]["description"]="The Australian government posts datasets online for public use, and there are currently 270 datasets available. All of the datasets which are a single file are also hosted here on infochimps, but this catalog contains basic metadata about all of the datasets available, including links to websites which can't be easily downloaded."
  File.new(File.join($outputdir,"catalog.icss.json"),'w').write(icss.inspect)
  #Always update the catalog icss.
end

task :run => [:process_pages, :catalog_icss]

Rake::Task[:run].invoke
